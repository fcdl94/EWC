{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import PIL.Image as Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "sample_size = 200\n",
    "hidden_size = 256\n",
    "num_task = 5\n",
    "epochs_interval = 1\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitMNIST(datasets.MNIST):\n",
    "    tasks = {\n",
    "        0: [0,1],\n",
    "        1: [2,3],\n",
    "        2: [4,5],\n",
    "        3: [6,7],\n",
    "        4: [8,9],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, root=\"/vandal/datasets\", train=True, task=0, cum=True):\n",
    "        super().__init__(root, train, download=True)\n",
    "        if not train and cum:\n",
    "            classes = [i for t in range(task + 1) for i in SplitMNIST.tasks[t]]\n",
    "        else:\n",
    "            classes = [i for i in SplitMNIST.tasks[task]]\n",
    "        self.idx = [i for i in range(len(self.targets)) if self.targets[i] in classes]\n",
    "        self.transform = transforms.ToTensor()\n",
    "        self.task = task\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[self.idx[index]], self.targets[self.idx[index]]\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        if self.train:\n",
    "            target = target - task*2\n",
    "        \n",
    "        return img.view(-1), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size=256, tasks=5, task_size=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.classifier = nn.ModuleList(\n",
    "            [nn.Linear(hidden_size, task_size) for _ in range(tasks)])\n",
    "\n",
    "    def forward(self, input, task):\n",
    "        x = F.relu(self.fc1(input))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.classifier[task](x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, input):\n",
    "        x = F.relu(self.fc1(input))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.cat([fc(x) for fc in self.classifier], dim=-1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineEWC(object):\n",
    "    def __init__(self, model, model_old, device, alpha=0.01, fisher=None):\n",
    "\n",
    "        self.model = model\n",
    "        self.model_old = model_old\n",
    "        self.model_old_dict = self.model_old.state_dict()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.fisher = {}\n",
    "        if fisher is not None: # initialize as old Fisher Matrix\n",
    "            self.fisher_old = fisher\n",
    "            for key in self.fisher_old:\n",
    "                self.fisher_old[key].requires_grad = False\n",
    "                self.fisher_old[key] = self.fisher_old[key].to(device)\n",
    "                self.fisher[key] = torch.zeros_like(fisher[key], device=device) \n",
    "        else: # initialize a new Fisher Matrix\n",
    "            self.fisher_old = None\n",
    "            self.fisher = {n:torch.zeros_like(p, device=device, requires_grad=False) \n",
    "                           for n, p in self.model.named_parameters() if p.requires_grad} \n",
    "            \n",
    "    def update(self, dataloader, task):\n",
    "        self.model.eval()\n",
    "        for input, target in dataloader:\n",
    "            self.model.zero_grad()\n",
    "            input = input.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            \n",
    "            output = self.model(input, task)\n",
    "                        \n",
    "            loss = F.cross_entropy(output, target) # Why they use entropy loss?\n",
    "            loss.backward()\n",
    "\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    self.fisher[n] += p.grad.data.clone().pow(2) / len(dataloader)\n",
    "    \n",
    "    def get_fisher(self):\n",
    "        return self.fisher # return the new Fisher matrix\n",
    "\n",
    "    def penalty(self):\n",
    "        loss = 0\n",
    "        if self.fisher_old is None:\n",
    "            return 0.\n",
    "        for n, p in self.model.named_parameters():\n",
    "            loss += (self.fisher_old[n] * (p - self.model_old_dict[n]).pow(2)).sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-20\n",
    "def normalize_fn(fisher):\n",
    "    return (fisher - fisher.min()) / (fisher.max() - fisher.min() + EPS)\n",
    "\n",
    "class EWCpp(object):\n",
    "    def __init__(self, model, model_old, device, alpha=0.9, fisher=None, normalize=True):\n",
    "\n",
    "        self.model = model\n",
    "        self.model_old = model_old\n",
    "        self.model_old_dict = self.model_old.state_dict()\n",
    "\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if fisher is not None: # initialize as old Fisher Matrix\n",
    "            self.fisher_old = fisher\n",
    "            for key in self.fisher_old:\n",
    "                self.fisher_old[key].requires_grad = False\n",
    "                self.fisher_old[key] = self.fisher_old[key].to(device)\n",
    "            self.fisher = deepcopy(fisher)\n",
    "            if normalize:\n",
    "                self.fisher_old = {n: normalize_fn(self.fisher_old[n]) for n in self.fisher_old}\n",
    "\n",
    "        else: # initialize a new Fisher Matrix\n",
    "            self.fisher_old = None\n",
    "            self.fisher = {n:torch.zeros_like(p, device=device, requires_grad=False) \n",
    "                           for n, p in self.model.named_parameters() if p.requires_grad} \n",
    "\n",
    "    def update(self):\n",
    "        # suppose model have already grad computed, so we can directly update the fisher by getting model.parameters\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                self.fisher[n] = (self.alpha * p.grad.data.pow(2)) + ((1-self.alpha)*self.fisher[n])\n",
    "\n",
    "    def get_fisher(self):\n",
    "        return self.fisher # return the new Fisher matrix\n",
    "\n",
    "    def penalty(self):\n",
    "        loss = 0\n",
    "        if self.fisher_old is None:\n",
    "            return 0.\n",
    "        for n, p in self.model.named_parameters():\n",
    "            loss += (self.fisher_old[n] * (p - self.model_old_dict[n]).pow(2)).sum()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    train_loader = {}\n",
    "    test_loader_no_cum = {}\n",
    "    test_loader = {}\n",
    "\n",
    "    for i in range(num_task):\n",
    "        train_loader[i] = torch.utils.data.DataLoader(SplitMNIST(train=True, task=i),\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      num_workers=4)\n",
    "        test_loader[i] = torch.utils.data.DataLoader(SplitMNIST(train=False, task=i),\n",
    "                                                     batch_size=batch_size)\n",
    "        test_loader_no_cum[i] = torch.utils.data.DataLoader(SplitMNIST(train=False, task=i, cum=False),\n",
    "                                                     batch_size=batch_size)\n",
    "    return train_loader, test_loader, test_loader_no_cum\n",
    "\n",
    "def test(model: nn.Module, data_loader: torch.utils.data.DataLoader):\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    size = float(0.)\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "        output = model.predict(input)\n",
    "        _, prediction = output.max(1)\n",
    "        correct += torch.sum(prediction.eq(target)).float()\n",
    "        size += len(target)\n",
    "    return correct / size\n",
    "\n",
    "train_loader, test_loader, test_loader_no_cum = get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_train(model, optimizer, data_loader, task):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input, task)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        epoch_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "def standard_process(model, epochs, task):\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = normal_train(model, optimizer, train_loader[task], task)\n",
    "        if epoch % epochs_interval == 0:\n",
    "            print(f\"Epoch {epoch + 1}: Loss {loss}\")\n",
    "\n",
    "    print(f\"Acc task {task} is {test(model, test_loader[task])}\")      \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Loss 0.026549091562628746\n",
      "Epoch 2: Loss 0.0031973535660654306\n",
      "Epoch 3: Loss 0.0021754407789558172\n",
      "Epoch 4: Loss 0.002085939748212695\n",
      "Epoch 5: Loss 0.0027679249178618193\n",
      "Epoch 6: Loss 0.0017462905962020159\n",
      "Epoch 7: Loss 0.0006506486097350717\n",
      "Epoch 8: Loss 0.0009288053261116147\n",
      "Epoch 9: Loss 0.00026676274137571454\n",
      "Epoch 10: Loss 0.0009421079885214567\n",
      "Acc task 0 is 0.9985815286636353\n",
      "\n",
      "Epoch 1: Loss 0.11990812420845032\n",
      "Epoch 2: Loss 0.03745390102267265\n",
      "Epoch 3: Loss 0.018878968432545662\n",
      "Epoch 4: Loss 0.011726373806595802\n",
      "Epoch 5: Loss 0.008084713481366634\n",
      "Epoch 6: Loss 0.006686045788228512\n",
      "Epoch 7: Loss 0.005309024825692177\n",
      "Epoch 8: Loss 0.004606361500918865\n",
      "Epoch 9: Loss 0.004533804953098297\n",
      "Epoch 10: Loss 0.0033910039346665144\n",
      "Acc task 1 is 0.7825354337692261\n",
      "\n",
      "Epoch 1: Loss 0.05751190707087517\n",
      "Epoch 2: Loss 0.0064352150075137615\n",
      "Epoch 3: Loss 0.0018309748265892267\n",
      "Epoch 4: Loss 0.0004091352748218924\n",
      "Epoch 5: Loss 0.00011812253796961159\n",
      "Epoch 6: Loss 5.470484757097438e-05\n",
      "Epoch 7: Loss 3.7413425161503255e-05\n",
      "Epoch 8: Loss 2.7493590096128173e-05\n",
      "Epoch 9: Loss 2.1024796296842396e-05\n",
      "Epoch 10: Loss 1.6518484699190594e-05\n",
      "Acc task 2 is 0.5940971374511719\n",
      "\n",
      "Epoch 1: Loss 0.023178070783615112\n",
      "Epoch 2: Loss 0.002700644079595804\n",
      "Epoch 3: Loss 0.000625460350420326\n",
      "Epoch 4: Loss 7.955793262226507e-05\n",
      "Epoch 5: Loss 3.6915967939421535e-05\n",
      "Epoch 6: Loss 2.0589925043168478e-05\n",
      "Epoch 7: Loss 1.2131038602092303e-05\n",
      "Epoch 8: Loss 7.140761681512231e-06\n",
      "Epoch 9: Loss 4.3080622162960935e-06\n",
      "Epoch 10: Loss 2.7415430849941913e-06\n",
      "Acc task 3 is 0.5268803834915161\n",
      "\n",
      "Epoch 1: Loss 0.0717194527387619\n",
      "Epoch 2: Loss 0.027435386553406715\n",
      "Epoch 3: Loss 0.017486052587628365\n",
      "Epoch 4: Loss 0.010234330780804157\n",
      "Epoch 5: Loss 0.0062921964563429356\n",
      "Epoch 6: Loss 0.0036412510089576244\n",
      "Epoch 7: Loss 0.0021589184179902077\n",
      "Epoch 8: Loss 0.002704814774915576\n",
      "Epoch 9: Loss 0.002108536195009947\n",
      "Epoch 10: Loss 0.0030966897029429674\n",
      "Acc task 4 is 0.3974999785423279\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "model = MLP().cuda()\n",
    "EPS = 1e-20\n",
    "\n",
    "for task in range(num_task):\n",
    "    print(\"\")\n",
    "    model = standard_process(model, epochs, task=task)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per task Acc\n",
      "0 : 0.283\n",
      "1 : 0.356\n",
      "2 : 0.585\n",
      "3 : 0.418\n",
      "4 : 0.366\n",
      "Cumulative Acc\n",
      "0.397\n"
     ]
    }
   ],
   "source": [
    "# PER TASK ACCURACY\n",
    "print(\"Per task Acc\")\n",
    "for t in range(num_task):\n",
    "    print(f\"{t} : {test(model, test_loader_no_cum[t]).item() :.3f}\")\n",
    "    \n",
    "# TOTAL ACCURACY\n",
    "print(\"Cumulative Acc\")\n",
    "print(f\"{test(model, test_loader[4]).item() :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online EWC\n",
    "From: https://arxiv.org/pdf/1805.06370.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_ewc_train(model, optimizer, data_loader, ewc, importance, task):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input, task)\n",
    "        \n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_ewc = importance * ewc.penalty()\n",
    "        if loss_ewc != 0.:\n",
    "            loss_ewc.backward()\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        \n",
    "        optimizer.step()\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def online_ewc_process(model, ewc, epochs, importance, task):\n",
    "\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = online_ewc_train(model, optimizer, train_loader[task], ewc, importance, task)\n",
    "        if epoch % epochs_interval == 0:\n",
    "            print(f\"Epoch {epoch + 1}: Loss {loss}\")\n",
    "    \n",
    "    ewc.update(train_loader[task], task)\n",
    "    # print({k:v.mean().item() for k,v in ewc.get_fisher().items()})\n",
    "    # print({k:v.max().item() for k,v in ewc.get_fisher().items()})\n",
    "\n",
    "    print(f\"Acc task {task} is {test(model, test_loader[task])}\")\n",
    "       \n",
    "    return model, ewc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Loss 0.026549091562628746\n",
      "Epoch 2: Loss 0.0031973535660654306\n",
      "Epoch 3: Loss 0.0021754407789558172\n",
      "Epoch 4: Loss 0.002085939748212695\n",
      "Epoch 5: Loss 0.0027679249178618193\n",
      "Epoch 6: Loss 0.0017462905962020159\n",
      "Epoch 7: Loss 0.0006506486097350717\n",
      "Epoch 8: Loss 0.0009288053261116147\n",
      "Epoch 9: Loss 0.00026676274137571454\n",
      "Epoch 10: Loss 0.0009421079885214567\n",
      "Acc task 0 is 0.9985815286636353\n",
      "\n",
      " New fisher (normalized):\n",
      "{'fc1.weight': (0.0, 2.158017391697066e-15, 1.0), 'fc1.bias': (0.0, 0.013740774244070053, 1.0), 'fc2.weight': (0.0, 3.183357193847769e-06, 1.0), 'fc2.bias': (0.0, 0.015340087935328484, 1.0), 'classifier.0.weight': (0.0, 0.0058732349425554276, 1.0), 'classifier.0.bias': (0.0, 0.0, 1.0), 'classifier.1.weight': (0.0, 0.0, 0.0), 'classifier.1.bias': (0.0, 0.0, 0.0), 'classifier.2.weight': (0.0, 0.0, 0.0), 'classifier.2.bias': (0.0, 0.0, 0.0), 'classifier.3.weight': (0.0, 0.0, 0.0), 'classifier.3.bias': (0.0, 0.0, 0.0), 'classifier.4.weight': (0.0, 0.0, 0.0), 'classifier.4.bias': (0.0, 0.0, 0.0)}\n",
      "\n",
      "Epoch 1: Loss 0.17751246690750122\n",
      "Epoch 2: Loss 0.05746307224035263\n",
      "Epoch 3: Loss 0.034907083958387375\n",
      "Epoch 4: Loss 0.02209271304309368\n",
      "Epoch 5: Loss 0.013987257145345211\n",
      "Epoch 6: Loss 0.009521249681711197\n",
      "Epoch 7: Loss 0.006559308618307114\n",
      "Epoch 8: Loss 0.00420741830021143\n",
      "Epoch 9: Loss 0.004026839975267649\n",
      "Epoch 10: Loss 0.002905000001192093\n",
      "Acc task 1 is 0.9061823487281799\n",
      "\n",
      " New fisher (normalized):\n",
      "{'fc1.weight': (0.0, 5.831612170936751e-08, 1.0), 'fc1.bias': (0.0, 0.0411982499063015, 1.0), 'fc2.weight': (0.0, 0.0001809179229894653, 1.0), 'fc2.bias': (0.0, 0.05798354744911194, 1.3416366577148438), 'classifier.0.weight': (0.0, 0.005579573102295399, 0.949999988079071), 'classifier.0.bias': (0.0, 0.0, 0.949999988079071), 'classifier.1.weight': (0.0, 0.029626132920384407, 1.0), 'classifier.1.bias': (0.0, 0.0, 1.0), 'classifier.2.weight': (0.0, 0.0, 0.0), 'classifier.2.bias': (0.0, 0.0, 0.0), 'classifier.3.weight': (0.0, 0.0, 0.0), 'classifier.3.bias': (0.0, 0.0, 0.0), 'classifier.4.weight': (0.0, 0.0, 0.0), 'classifier.4.bias': (0.0, 0.0, 0.0)}\n",
      "\n",
      "Epoch 1: Loss 0.22306418418884277\n",
      "Epoch 2: Loss 0.024431321769952774\n",
      "Epoch 3: Loss 0.01291802991181612\n",
      "Epoch 4: Loss 0.007460265886038542\n",
      "Epoch 5: Loss 0.00439744908362627\n",
      "Epoch 6: Loss 0.0028099501505494118\n",
      "Epoch 7: Loss 0.0025299617554992437\n",
      "Epoch 8: Loss 0.0018012399086728692\n",
      "Epoch 9: Loss 0.0024268904235213995\n",
      "Epoch 10: Loss 0.0006011264631524682\n",
      "Acc task 2 is 0.8381694555282593\n",
      "\n",
      " New fisher (normalized):\n",
      "{'fc1.weight': (0.0, 3.0573296498914715e-06, 1.0), 'fc1.bias': (0.0, 0.051483746618032455, 1.0), 'fc2.weight': (0.0, 0.0008334177546203136, 1.0142141580581665), 'fc2.bias': (0.0, 0.11241758614778519, 1.3286254405975342), 'classifier.0.weight': (0.0, 0.0053005944937467575, 0.9024999737739563), 'classifier.0.bias': (0.0, 0.0, 0.9024999737739563), 'classifier.1.weight': (0.0, 0.028144825249910355, 0.949999988079071), 'classifier.1.bias': (0.0, 0.0, 0.949999988079071), 'classifier.2.weight': (0.0, 0.048591163009405136, 1.0), 'classifier.2.bias': (0.0, 0.0, 1.0), 'classifier.3.weight': (0.0, 0.0, 0.0), 'classifier.3.bias': (0.0, 0.0, 0.0), 'classifier.4.weight': (0.0, 0.0, 0.0), 'classifier.4.bias': (0.0, 0.0, 0.0)}\n",
      "\n",
      "Epoch 1: Loss 0.21493470668792725\n",
      "Epoch 2: Loss 0.10754188895225525\n",
      "Epoch 3: Loss 0.06643962115049362\n",
      "Epoch 4: Loss 0.020633015781641006\n",
      "Epoch 5: Loss 0.015749847516417503\n",
      "Epoch 6: Loss 0.013105222955346107\n",
      "Epoch 7: Loss 0.011280856095254421\n",
      "Epoch 8: Loss 0.009919686242938042\n",
      "Epoch 9: Loss 0.008867790922522545\n",
      "Epoch 10: Loss 0.008064402267336845\n",
      "Acc task 3 is 0.6997630000114441\n",
      "\n",
      " New fisher (normalized):\n",
      "{'fc1.weight': (0.0, 8.768662155489437e-06, 1.0000003576278687), 'fc1.bias': (0.0, 0.050267867743968964, 1.000006079673767), 'fc2.weight': (0.0, 0.0013197548687458038, 1.4807689189910889), 'fc2.bias': (0.0, 0.17565412819385529, 2.0083320140838623), 'classifier.0.weight': (0.0, 0.00503556476905942, 0.8573749661445618), 'classifier.0.bias': (0.0, 0.0, 0.8573749661445618), 'classifier.1.weight': (0.0, 0.026737583801150322, 0.9024999737739563), 'classifier.1.bias': (0.0, 0.0, 0.9024999737739563), 'classifier.2.weight': (0.0, 0.046161603182554245, 0.949999988079071), 'classifier.2.bias': (0.0, 0.0, 0.949999988079071), 'classifier.3.weight': (0.0, 0.0631459578871727, 1.0), 'classifier.3.bias': (0.0, 0.0, 1.0), 'classifier.4.weight': (0.0, 0.0, 0.0), 'classifier.4.bias': (0.0, 0.0, 0.0)}\n",
      "\n",
      "Epoch 1: Loss 0.20598718523979187\n",
      "Epoch 2: Loss 0.0789179876446724\n",
      "Epoch 3: Loss 0.06054508686065674\n",
      "Epoch 4: Loss 0.053225647658109665\n",
      "Epoch 5: Loss 0.048899151384830475\n",
      "Epoch 6: Loss 0.04582274705171585\n",
      "Epoch 7: Loss 0.04344920814037323\n",
      "Epoch 8: Loss 0.04150525480508804\n",
      "Epoch 9: Loss 0.039892103523015976\n",
      "Epoch 10: Loss 0.038516268134117126\n",
      "Acc task 4 is 0.5386999845504761\n",
      "\n",
      " New fisher (normalized):\n",
      "{'fc1.weight': (0.0, 1.6898949979804456e-05, 1.0), 'fc1.bias': (0.0, 0.048604775220155716, 1.0000001192092896), 'fc2.weight': (0.0, 0.002253109123557806, 1.817780613899231), 'fc2.bias': (0.0, 0.24770671129226685, 2.2111384868621826), 'classifier.0.weight': (0.0, 0.004783786367624998, 0.8145062327384949), 'classifier.0.bias': (0.0, 0.0, 0.8145062327384949), 'classifier.1.weight': (0.0, 0.02540070377290249, 0.8573749661445618), 'classifier.1.bias': (0.0, 0.0, 0.8573749661445618), 'classifier.2.weight': (0.0, 0.0438535213470459, 0.9024999737739563), 'classifier.2.bias': (0.0, 0.0, 0.9024999737739563), 'classifier.3.weight': (0.0, 0.059988658875226974, 0.949999988079071), 'classifier.3.bias': (0.0, 0.0, 0.949999988079071), 'classifier.4.weight': (0.0, 0.11834362149238586, 1.0), 'classifier.4.bias': (0.0, 0.0, 1.0)}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "model = MLP().cuda()\n",
    "fisher = None\n",
    "importance = 75000\n",
    "EPS = 1e-20\n",
    "phi = 0.95\n",
    "\n",
    "for task in range(num_task):\n",
    "    model_old = deepcopy(model)\n",
    "    for p in model_old.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    print(\"\")\n",
    "    ewc = OnlineEWC(model, model_old, \"cuda\", fisher=fisher)\n",
    "    model, ewc = online_ewc_process(model, ewc, epochs, task=task, importance=importance)\n",
    "    \n",
    "    if fisher is None:\n",
    "        fisher = deepcopy(ewc.get_fisher())\n",
    "       \n",
    "        fisher = {n: (fisher[n] - fisher[n].min()) / (fisher[n].max() - fisher[n].min() + EPS) for n in fisher}\n",
    "        \n",
    "        print(\"\\n New fisher (normalized):\")\n",
    "        print({n:(p.min().item(), p.median().item(), p.max().item()) for n,p in fisher.items()})\n",
    "    else:\n",
    "        new_fisher = ewc.get_fisher()\n",
    "        for n in fisher:\n",
    "            new_fisher[n] = (new_fisher[n] - new_fisher[n].min()) / (new_fisher[n].max() - new_fisher[n].min() + EPS)\n",
    "            fisher[n] = phi*fisher[n] + new_fisher[n]\n",
    "        print(\"\\n New fisher (normalized):\")\n",
    "        print({n:(p.min().item(), p.median().item(), p.max().item()) for n,p in fisher.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per task Acc\n",
      "0 : 0.908\n",
      "1 : 0.663\n",
      "2 : 0.737\n",
      "3 : 0.266\n",
      "4 : 0.102\n",
      "Cumulative Acc\n",
      "0.539\n"
     ]
    }
   ],
   "source": [
    "# PER TASK ACCURACY\n",
    "print(\"Per task Acc\")\n",
    "for t in range(num_task):\n",
    "    print(f\"{t} : {test(model, test_loader_no_cum[t]).item() :.3f}\")\n",
    "    \n",
    "# TOTAL ACCURACY\n",
    "print(\"Cumulative Acc\")\n",
    "print(f\"{test(model, test_loader[4]).item() :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC ++\n",
    "From: http://openaccess.thecvf.com/content_ECCV_2018/papers/Arslan_Chaudhry__Riemannian_Walk_ECCV_2018_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewcpp_train(model, optimizer, data_loader, ewc, importance, task):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input, task)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        epoch_loss += loss\n",
    "        loss.backward()\n",
    "        ewc.update()\n",
    "        loss_ewc = importance * ewc.penalty()\n",
    "        if loss_ewc != 0:\n",
    "            loss_ewc.backward()\n",
    "        optimizer.step()\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "def ewcpp_process(model, ewc, epochs, importance, task):\n",
    "\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = ewcpp_train(model, optimizer, train_loader[task], ewc, importance, task)\n",
    "        if epoch % epochs_interval == 0:\n",
    "            print(f\"Epoch {epoch + 1}: Loss {loss}\")\n",
    "\n",
    "    print(f\"Acc task {task} is {test(model, test_loader[task])}\")\n",
    "\n",
    "    return model, ewc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Loss 0.026549091562628746\n",
      "Epoch 2: Loss 0.0031973535660654306\n",
      "Epoch 3: Loss 0.0021754407789558172\n",
      "Epoch 4: Loss 0.002085939748212695\n",
      "Epoch 5: Loss 0.0027679249178618193\n",
      "Epoch 6: Loss 0.0017462905962020159\n",
      "Epoch 7: Loss 0.0006506486097350717\n",
      "Epoch 8: Loss 0.0009288053261116147\n",
      "Epoch 9: Loss 0.00026676274137571454\n",
      "Epoch 10: Loss 0.0009421079885214567\n",
      "Acc task 0 is 0.9985815286636353\n",
      "\n",
      " New fisher (not normalized):\n",
      "{'fc1.weight': (0.0, 0.0, 1.955451356394633e-09), 'fc1.bias': (0.0, 1.7514060016312174e-15, 1.9745394208570133e-09), 'fc2.weight': (0.0, 1.1023583941414215e-22, 5.509139255899242e-10), 'fc2.bias': (0.0, 1.504856352377293e-14, 1.5372128570056987e-10), 'classifier.0.weight': (0.0, 2.1230128657671876e-13, 1.3325110792550277e-08), 'classifier.0.bias': (3.8616163600124764e-09, 3.8616163600124764e-09, 3.863255493286033e-09), 'classifier.1.weight': (0.0, 0.0, 0.0), 'classifier.1.bias': (0.0, 0.0, 0.0), 'classifier.2.weight': (0.0, 0.0, 0.0), 'classifier.2.bias': (0.0, 0.0, 0.0), 'classifier.3.weight': (0.0, 0.0, 0.0), 'classifier.3.bias': (0.0, 0.0, 0.0), 'classifier.4.weight': (0.0, 0.0, 0.0), 'classifier.4.bias': (0.0, 0.0, 0.0)}\n",
      "\n",
      "Epoch 1: Loss 0.1299038529396057\n",
      "Epoch 2: Loss 0.044987525790929794\n",
      "Epoch 3: Loss 0.02462279051542282\n",
      "Epoch 4: Loss 0.013874784111976624\n",
      "Epoch 5: Loss 0.0097681088373065\n",
      "Epoch 6: Loss 0.007740178611129522\n",
      "Epoch 7: Loss 0.007820854894816875\n",
      "Epoch 8: Loss 0.005181392654776573\n",
      "Epoch 9: Loss 0.0030947881750762463\n",
      "Epoch 10: Loss 0.00292657851241529\n",
      "Acc task 1 is 0.912917971611023\n",
      "\n",
      " New fisher (not normalized):\n",
      "{'fc1.weight': (0.0, 4.224411664369281e-31, 3.956149541295417e-08), 'fc1.bias': (0.0, 6.778651424899085e-12, 4.001827491606491e-08), 'fc2.weight': (0.0, 1.8637007593387458e-15, 2.622040717881191e-08), 'fc2.bias': (0.0, 3.62082759608473e-11, 1.6984642581263643e-09), 'classifier.0.weight': (0.0, 0.0, 0.0), 'classifier.0.bias': (0.0, 0.0, 0.0), 'classifier.1.weight': (0.0, 9.102500975188832e-09, 9.982038307043695e-08), 'classifier.1.bias': (8.827848674286543e-09, 8.827848674286543e-09, 8.829124986675652e-09), 'classifier.2.weight': (0.0, 0.0, 0.0), 'classifier.2.bias': (0.0, 0.0, 0.0), 'classifier.3.weight': (0.0, 0.0, 0.0), 'classifier.3.bias': (0.0, 0.0, 0.0), 'classifier.4.weight': (0.0, 0.0, 0.0), 'classifier.4.bias': (0.0, 0.0, 0.0)}\n",
      "\n",
      "Epoch 1: Loss 0.08410917967557907\n",
      "Epoch 2: Loss 0.00878722034394741\n",
      "Epoch 3: Loss 0.004031104035675526\n",
      "Epoch 4: Loss 0.0021365778520703316\n",
      "Epoch 5: Loss 0.0009011916117742658\n",
      "Epoch 6: Loss 0.00034336847602389753\n",
      "Epoch 7: Loss 0.00013928009138908237\n",
      "Epoch 8: Loss 0.00010325458424631506\n",
      "Epoch 9: Loss 8.064709982136264e-05\n",
      "Epoch 10: Loss 6.500325253000483e-05\n",
      "Acc task 2 is 0.7701873779296875\n",
      "\n",
      " New fisher (not normalized):\n",
      "{'fc1.weight': (0.0, 1.2558669462978397e-27, 3.4077217492267664e-08), 'fc1.bias': (0.0, 2.8624097883622746e-11, 2.969561307963886e-08), 'fc2.weight': (0.0, 1.3242027916901544e-13, 2.676850074934123e-09), 'fc2.bias': (0.0, 2.5114462592901177e-11, 3.1990826654393345e-10), 'classifier.0.weight': (0.0, 0.0, 0.0), 'classifier.0.bias': (0.0, 0.0, 0.0), 'classifier.1.weight': (0.0, 0.0, 0.0), 'classifier.1.bias': (0.0, 0.0, 0.0), 'classifier.2.weight': (0.0, 2.9219477948316808e-09, 5.618159093501163e-08), 'classifier.2.bias': (5.5411875088395846e-09, 5.5411875088395846e-09, 5.5441762292218755e-09), 'classifier.3.weight': (0.0, 0.0, 0.0), 'classifier.3.bias': (0.0, 0.0, 0.0), 'classifier.4.weight': (0.0, 0.0, 0.0), 'classifier.4.bias': (0.0, 0.0, 0.0)}\n",
      "\n",
      "Epoch 1: Loss 0.04023308679461479\n",
      "Epoch 2: Loss 0.004565201699733734\n",
      "Epoch 3: Loss 0.002458336530253291\n",
      "Epoch 4: Loss 0.0010366302449256182\n",
      "Epoch 5: Loss 0.00053099833894521\n",
      "Epoch 6: Loss 0.00023650670482311398\n",
      "Epoch 7: Loss 0.00015131758118513972\n",
      "Epoch 8: Loss 0.00010839755123015493\n",
      "Epoch 9: Loss 8.198518480639905e-05\n",
      "Epoch 10: Loss 6.384488369803876e-05\n",
      "Acc task 3 is 0.620930552482605\n",
      "\n",
      " New fisher (not normalized):\n",
      "{'fc1.weight': (0.0, 2.5977870617756254e-26, 6.0718630644218674e-12), 'fc1.bias': (0.0, 1.605135481859898e-15, 6.196528099661203e-12), 'fc2.weight': (0.0, 3.4591417290872754e-18, 2.2112413896463456e-12), 'fc2.bias': (0.0, 1.738557888249909e-15, 2.0507686803068748e-13), 'classifier.0.weight': (0.0, 0.0, 0.0), 'classifier.0.bias': (0.0, 0.0, 0.0), 'classifier.1.weight': (0.0, 0.0, 0.0), 'classifier.1.bias': (0.0, 0.0, 0.0), 'classifier.2.weight': (0.0, 0.0, 0.0), 'classifier.2.bias': (0.0, 0.0, 0.0), 'classifier.3.weight': (0.0, 5.312071854439437e-13, 4.767015254758533e-11), 'classifier.3.bias': (2.7092954615892673e-12, 2.7092954615892673e-12, 2.8026292224481164e-12), 'classifier.4.weight': (0.0, 0.0, 0.0), 'classifier.4.bias': (0.0, 0.0, 0.0)}\n",
      "\n",
      "Epoch 1: Loss 0.07679717987775803\n",
      "Epoch 2: Loss 0.03457625210285187\n",
      "Epoch 3: Loss 0.02283237688243389\n",
      "Epoch 4: Loss 0.01569165103137493\n",
      "Epoch 5: Loss 0.011857445351779461\n",
      "Epoch 6: Loss 0.00730839092284441\n",
      "Epoch 7: Loss 0.00463677616789937\n",
      "Epoch 8: Loss 0.003172383876517415\n",
      "Epoch 9: Loss 0.0022312565706670284\n",
      "Epoch 10: Loss 0.002137048402801156\n",
      "Acc task 4 is 0.5228999853134155\n",
      "\n",
      " New fisher (not normalized):\n",
      "{'fc1.weight': (0.0, 1.0242877371808741e-26, 4.346095465734834e-07), 'fc1.bias': (0.0, 1.2233858370791495e-10, 4.387010221762466e-07), 'fc2.weight': (0.0, 1.0847055675541895e-12, 7.527298606646582e-08), 'fc2.bias': (0.0, 1.1227609814090656e-10, 3.6478524645389143e-09), 'classifier.0.weight': (0.0, 0.0, 0.0), 'classifier.0.bias': (0.0, 0.0, 0.0), 'classifier.1.weight': (0.0, 0.0, 0.0), 'classifier.1.bias': (0.0, 0.0, 0.0), 'classifier.2.weight': (0.0, 0.0, 0.0), 'classifier.2.bias': (0.0, 0.0, 0.0), 'classifier.3.weight': (0.0, 0.0, 0.0), 'classifier.3.bias': (0.0, 0.0, 0.0), 'classifier.4.weight': (0.0, 1.3463240122746356e-07, 2.6088130198331783e-06), 'classifier.4.bias': (6.118778372865563e-08, 6.118778372865563e-08, 6.119115170122313e-08)}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "model = MLP().cuda()\n",
    "fisher = None\n",
    "importance = 75000\n",
    "EPS = 1e-20\n",
    "alpha = 0.9\n",
    "normalize = True\n",
    "\n",
    "for task in range(num_task):\n",
    "    model_old = deepcopy(model)\n",
    "    for p in model_old.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    print(\"\")\n",
    "    ewc = EWCpp(model, model_old, \"cuda\", fisher=fisher, alpha=alpha, normalize=normalize)\n",
    "    model, ewc = ewcpp_process(model, ewc, epochs, task=task, importance=importance)\n",
    "\n",
    "    fisher = deepcopy(ewc.get_fisher())\n",
    "\n",
    "    print(\"\\n New fisher (not normalized):\")\n",
    "    print({n:(p.min().item(), p.median().item(), p.max().item()) for n,p in fisher.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per task Acc\n",
      "0 : 0.468\n",
      "1 : 0.667\n",
      "2 : 0.747\n",
      "3 : 0.364\n",
      "4 : 0.380\n",
      "Cumulative Acc\n",
      "0.523\n"
     ]
    }
   ],
   "source": [
    "# PER TASK ACCURACY\n",
    "print(\"Per task Acc\")\n",
    "for t in range(num_task):\n",
    "    print(f\"{t} : {test(model, test_loader_no_cum[t]).item() :.3f}\")\n",
    "    \n",
    "# TOTAL ACCURACY\n",
    "print(\"Cumulative Acc\")\n",
    "print(f\"{test(model, test_loader[4]).item() :.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
